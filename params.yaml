TrainingArguments:
  num_train_epochs: 1  # Reduce from 3 to 1 for testing
  warmup_steps: 100    # Reduce from 500
  per_device_train_batch_size: 4  # Increase from 2 if GPU memory allows
  per_device_eval_batch_size: 4   # Increase from 2
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: steps
  eval_steps: 100      # Reduce from 500 for faster feedback
  save_steps: 500      # Reduce from 1000
  gradient_accumulation_steps: 4  # Reduce from 8
  fp16: true
  dataloader_pin_memory: true     # Change to true for GPU
  dataloader_num_workers: 2       # Increase from 0
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  report_to: []
  output_dir: artifacts/model_trainer
  max_steps: 1000  # ADD THIS - limit total steps for testing
